{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49f5b8c4",
   "metadata": {},
   "source": [
    "# <font color='HotPink'>**Data Mining Project 2025/2026 - Group 94** <br></font> Amazing International Airlines Inc.\n",
    "\n",
    "\n",
    "**Diogo Henrique Lopes de Carvalho** ***(20221935)***\n",
    "\n",
    "**Maria Luiza Salum Alves Corrêa** ***(20221902)***\n",
    "\n",
    "**Ricardo Miguel Silva Martins Pereira** ***(20250343)***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d30a18",
   "metadata": {},
   "source": [
    "*** \n",
    "\n",
    "## **Index**\n",
    "\n",
    "***First Delivery***\n",
    "1. [Business Understanding](#1)\n",
    "1. [Imports](#2)\n",
    "1. [Data Understanding](#3)\n",
    "    1. [The Dataset](#3_1)\n",
    "1. [Data Preparation](#4)\n",
    "    1. [Data Cleaning](#4_1)\n",
    "    1. [Variable Classification](#4_2)\n",
    "    1. [Summary Statistics](#4_3)\n",
    "    1. [Data Visualisation](#4_4)\n",
    "    1. [Feature Engineering](#4_5)\n",
    "\n",
    "***Second Delivery***\n",
    "\n",
    "5. [Clustering](#5)\n",
    "5. [Evaluation](#6)\n",
    "5. [Suggestions](#7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bf29f9",
   "metadata": {},
   "source": [
    "#\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39191679",
   "metadata": {},
   "source": [
    "## 1. <a id=\"1\">Business Understanding</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9308b45",
   "metadata": {},
   "source": [
    "Our client, **Amazing International Airlines Inc.**, tasked our consulting team with the analysis of their customer loyalty membership data and corresponding flight activity collected over a three-year period. <br>\n",
    "\n",
    "The goal of our analysis is to create useful insights through a segmentation process that consists of grouping and viewing the available data by different lenses; such as *economic contribution*, *behaviours in purchases and travels*, and *demographic divisions*. <br>\n",
    "\n",
    "After sharing the insights, our team will evaluate the results and give AIAI suggestions on how to adapt their services, rewards and communications to meet the needs of the most relevant groups found."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c451d459",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf222b56",
   "metadata": {},
   "source": [
    "## 2. <a id=\"2\">Imports</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ec72c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from scipy import stats\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "# import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacdb365",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d755bf7",
   "metadata": {},
   "source": [
    "## 3. <a id=\"3\">Data Understanding</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22e1a63",
   "metadata": {},
   "source": [
    "### 1. <a id=\"3_1\">The Dataset</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68da9ac",
   "metadata": {},
   "source": [
    "The dataset given to our team consists of three `csv` files:\n",
    "\n",
    "- `DM_AIAI_CustomerDB.csv`\n",
    "\n",
    "*Three (3) years of data related to Customers in the Loyalty Program*\n",
    "- `DM_AIAI_FlightsDB.csv`\n",
    "\n",
    "*Three (3) years of data related to Flights of the Loyalty Program Members*\n",
    "- `DM_AIAI_Metadata.csv`\n",
    "\n",
    "*Description of the Variables in the CustomerDB and FlightDB Files*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96385bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDB = pd.read_csv(r'data\\DM_AIAI_CustomerDB.csv', sep=',')\n",
    "flightDB = pd.read_csv(r'data\\DM_AIAI_FlightsDB.csv', sep=',')\n",
    "metadata = pd.read_csv(r'data\\DM_AIAI_Metadata.csv', sep=';')\n",
    "\n",
    "customerDB_copy = customerDB.copy()\n",
    "flightDB_copy = flightDB.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3c988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)  # To see full descriptions\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78806286",
   "metadata": {},
   "source": [
    "#### Initial Conclusions\n",
    "*What conclusions can we make from the metadata?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acab1ebe",
   "metadata": {},
   "source": [
    "##### Customer DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc649c23",
   "metadata": {},
   "source": [
    "**Useful:**\n",
    "- `Loyalty#`: Will be used. Connects Customer's and Flights' information.\n",
    "\n",
    "**Will be Dropped:**\n",
    "- `First Name`, `Last Name`, `Customer Name`: First two are redundant since we have the full name, but this information is irrelevant for clustering anyways. We'll drop everything after data cleaning. *(Keeping `Customer Name` for now to check something with the IDs)*\n",
    "- `Latitude`, `Longitude`, `Postal Code`: No need for specific customer location. *(Spoilers: We checked, some coordinates lead to nowhere and just make no sense overall)*\n",
    "\n",
    "**No Conclusions:** <br>\n",
    "*May be relevant. No conclusions about data quality from metadata.*\n",
    "- `Country`\n",
    "- `Province or State`\n",
    "- `City`\n",
    "- `Postal Code`\n",
    "- `Gender`\n",
    "- `Education`\n",
    "- `Location Code`\n",
    "- `Income`\n",
    "- `Marital Status`\n",
    "- `Loyalty Status`\n",
    "- `EnrollmentDataOpening`\n",
    "- `CancellationDate`\n",
    "- `Customer Lifetime Value`\n",
    "- `EnrollmentType`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dd66cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDB_copy.drop(columns=['First Name', 'Last Name', 'Latitude', 'Longitude', 'Postal code'], inplace=True)\n",
    "customerDB_copy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3e6ec4",
   "metadata": {},
   "source": [
    "From the non-null count, we can see that there are only three variables with missing values: `Income`, `CancellationDate` and `Customer Lifetime Value`. Having null values in `CancellationDate` clearly makes sense as we don't expect all customers in the dataset to have cancelled their membership. For `Income` and `Customer Lifetime Value`, we'll try to understand the reason later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9518117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's a random column with no name, let's drop it\n",
    "customerDB_copy.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaed8658",
   "metadata": {},
   "source": [
    "##### Flights DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62ddbd2",
   "metadata": {},
   "source": [
    "**Useful:**\n",
    "- `Loyalty#`: Will be used. Connects Customer's and Flights' information.\n",
    "\n",
    "**Will be Dropped:**\n",
    "- `YearMonthDate`: Contains the same info as `Month` and `Year` with a constant date.\n",
    "- `DollarCostPointsRedeemed`: Redundant. `PointsRedeemed` gives the same information.\n",
    "\n",
    "**No Conclusions:** <br>\n",
    "*May be relevant. No conclusions about data quality from metadata.*\n",
    "- `Year`\n",
    "- `Month`\n",
    "- `NumFlights`\n",
    "- `NumFlightsWithCompanions`*\n",
    "- `DistanceKM`\n",
    "- `PointsAccumulated`\n",
    "- `PointsRedeemed`\n",
    "\n",
    "**We will assume `NumFlightsWithCompanions` is also a \"count by month\" even though it's not mentioned in the metadata.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f593770",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightDB_copy.drop(columns=['YearMonthDate', 'DollarCostPointsRedeemed'], inplace=True)\n",
    "flightDB_copy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d550ab",
   "metadata": {},
   "source": [
    "There are no null values in any variable. We'll take a closer look into the dataset when preparing the data. There might be missing values that are non-null since they could look like \"-\", \"Null\", \"Missing\", etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff663c5",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8342c42",
   "metadata": {},
   "source": [
    "## 4. <a id=\"4\">Data Preparation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436a05ca",
   "metadata": {},
   "source": [
    "There's a possibility of having some rows that the only plausible option is to remove from the dataset due to uncorrectable errors. For this reason, we are going to define variables to keep track of how many rows we are dropping.\n",
    "\n",
    "We are going to assume that an acceptable threshold is 5% of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdfa2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_dropped_rows = 0\n",
    "customer_original_rows = customerDB.shape[0]\n",
    "customer_dropped_percentage = 0\n",
    "customer_available_to_drop = (customer_original_rows * 0.05) - customer_dropped_rows\n",
    "print(f\"Available rows to drop in customerDB: {customer_available_to_drop} ({(customer_available_to_drop/customer_original_rows)*100:.0f}%)\")\n",
    "\n",
    "flight_dropped_rows = 0\n",
    "flight_original_rows = flightDB.shape[0]\n",
    "flight_dropped_percentage = 0\n",
    "flight_available_to_drop = (flight_original_rows * 0.05) - flight_dropped_rows\n",
    "print(f\"Available rows to drop in flightDB: {flight_available_to_drop} ({(flight_available_to_drop/flight_original_rows)*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ee83f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDB_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df9d0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing the different values in each variable and if they're less than 15, printing them bellow\n",
    "for variable in customerDB_copy.columns:\n",
    "    print(f\"{variable}: {customerDB_copy[variable].nunique()} unique values\\n\")\n",
    "    if customerDB_copy[variable].nunique() < 15:\n",
    "        print(f\"{customerDB_copy[variable].value_counts()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe02475",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightDB_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c8a3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing the different values in each variable and if they're less than 15, printing them bellow\n",
    "for variable in flightDB_copy.columns:\n",
    "    print(f\"{variable}: {flightDB_copy[variable].nunique()} unique values\\n\")\n",
    "    if flightDB_copy[variable].nunique() < 15:\n",
    "        print(f\"{flightDB_copy[variable].value_counts()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bae7bb",
   "metadata": {},
   "source": [
    "### 1. <a id=\"4_1\">Data Cleaning</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b2df38",
   "metadata": {},
   "source": [
    "##### Loyalty# *(+ Customer Name)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc062c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming how many rows have repeated Loyalty#\n",
    "print(customerDB_copy['Loyalty#'].duplicated().sum())\n",
    "# Confirming if the whole row is duplicated\n",
    "print(customerDB_copy.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfa306f",
   "metadata": {},
   "source": [
    "We kept `Customer Name` before so we could check if they're the same for repeated Loyalty IDs, but this is not the case as the unique values count is 16921. This means that we cannot use the name to fix the issue with duplicated IDs (they're not the same person).\n",
    "\n",
    "We could assign new random values to the repeated IDs, but the FlightsDB dataset is connected to each customer through this variable, so it makes no sense to change it in the CustomerDB since this would mean that they have no associated flights.\n",
    "\n",
    "We'll now drop `Customer Name` and remove the rows with repeated Loyalty ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64be3aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Customer Name\n",
    "customerDB_copy.drop(columns=['Customer Name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998ee12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only one of the duplicated rows and adding the number of dropped rows to the counter\n",
    "for loyalty_id in customerDB_copy['Loyalty#'][customerDB_copy['Loyalty#'].duplicated()]: # for each duplicated Loyalty#\n",
    "    duplicated_rows = customerDB_copy[customerDB_copy['Loyalty#'] == loyalty_id] # getting the duplicated rows\n",
    "    customerDB_copy.drop(duplicated_rows.index[1:], inplace=True) # dropping from the second to the last (just in case there are more than 2)\n",
    "    customer_dropped_rows += len(duplicated_rows) - 1 # adding the number of dropped rows to the counter\n",
    "    \n",
    "customer_dropped_percentage = customer_dropped_rows / customer_original_rows * 100\n",
    "customer_available_to_drop -= customer_dropped_rows\n",
    "print(f\"Total dropped rows: {customer_dropped_rows}, which is {customer_dropped_percentage:.2f}% of the original dataset. We can still drop {customer_available_to_drop} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e32401",
   "metadata": {},
   "source": [
    "##### Country"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bffce4",
   "metadata": {},
   "source": [
    "`Country` is a constant feature (Canada). We'll drop this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075fc7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Country \n",
    "customerDB_copy.drop(columns=['Country'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747a3c19",
   "metadata": {},
   "source": [
    "#### City"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51974ff8",
   "metadata": {},
   "source": [
    "Checking if all cities in the list are Canadian cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f28ba0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDB_copy['City'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccd1eb5",
   "metadata": {},
   "source": [
    "#### Income - Customer Lifetime Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afa1790",
   "metadata": {},
   "source": [
    "We'll see if the null values in `Income` are related to the null values in `Customer Lifetime Value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b692bf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_null = customerDB_copy[customerDB_copy['Income'].isnull()]\n",
    "clv_null = customerDB_copy[customerDB_copy['Customer Lifetime Value'].isnull()]\n",
    "income_clv_null = customerDB_copy[(customerDB_copy['Income'].isnull()) & (customerDB_copy['Customer Lifetime Value'].isnull())]\n",
    "print(f\"Income null values: {len(income_null)}\\nCLV null values: {len(clv_null)}\\nBoth null values: {len(income_clv_null)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7114a025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeing these customers in customerDB_copy\n",
    "customerDB_copy.loc[income_clv_null.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578da0cb",
   "metadata": {},
   "source": [
    "Now we know they're the same customers.\n",
    "\n",
    "Also, we can see that each one of these customers have the same `EnrollmentDateOpening` and `CancellationDate`, which makes us believe that they will have no relevant information in the flight dataset. If they don't, we'll just drop the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02883850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeing if there are any Loyalty# in customerDB_copy that are not in flightDB_copy\n",
    "missing_loyalty = customerDB_copy.index.difference(flightDB_copy['Loyalty#'].unique())\n",
    "# seeing if they're the ones with null values in Customer Lifetime Value\n",
    "clv_null_loyalty = customerDB_copy[customerDB_copy['Customer Lifetime Value'].isnull()].index\n",
    "meets = []\n",
    "for index in missing_loyalty:\n",
    "    if index in clv_null_loyalty:\n",
    "        meets.append(index)\n",
    "print(f'Loyalty# missing in flightDB_copy and null CLV/Income: \\n{meets}\\nTotal: {len(meets)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0210375",
   "metadata": {},
   "source": [
    "As expected, they have no flight information at all! Let's drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748f1be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDB_copy.drop(index=meets, inplace=True)\n",
    "customer_dropped_rows += len(meets)\n",
    "customer_dropped_percentage = customer_dropped_rows / customer_original_rows * 100\n",
    "customer_available_to_drop -= len(meets)\n",
    "print(f\"Total dropped rows: {customer_dropped_rows}, which is {customer_dropped_percentage:.2f}% of the original dataset. We can still drop {customer_available_to_drop} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8903bf2",
   "metadata": {},
   "source": [
    "Let's see the values equal to zero..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0f0389",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_zero = customerDB_copy[customerDB_copy['Income'] == 0]\n",
    "clv_zero = customerDB_copy[customerDB_copy['Customer Lifetime Value'] == 0]\n",
    "income_clv_zero = customerDB_copy[(customerDB_copy['Income'] == 0) & (customerDB_copy['Customer Lifetime Value'] == 0)]\n",
    "print(f\"Income zero values: {len(income_zero)}\\nCLV zero values: {len(clv_zero)}\\nBoth zero values: {len(income_clv_zero)}\\n\")\n",
    "print(income_zero['Customer Lifetime Value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382fd23f",
   "metadata": {},
   "source": [
    "We'll try to understand if there's a pattern for the 0€ income values in the Data Visualisation section. Maybe a specific type of customer decided to keep their income a secret..? We don't know yet, but we can see that these customers, even though they have no revealed `Income`, have an estimated CLV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e086725",
   "metadata": {},
   "source": [
    "#### CancellationDate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b972ed",
   "metadata": {},
   "source": [
    "Checking if there are dates in `CancellationDate` that are earlier than dates in `EnrollmentDateOpening`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6508cfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDB_copy['EnrollmentDateOpening'] = pd.to_datetime(customerDB_copy['EnrollmentDateOpening'], errors='coerce')\n",
    "customerDB_copy['CancellationDate'] = pd.to_datetime(customerDB_copy['CancellationDate'], errors='coerce')\n",
    "invalid_dates = customerDB_copy[customerDB_copy['CancellationDate'] <= customerDB_copy['EnrollmentDateOpening']]\n",
    "print(f\"Number of rows with CancellationDate before EnrollmentDateOpening: {len(invalid_dates)}\")\n",
    "invalid_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6a945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing the total flight dataset data per customer in the invalid_dates\n",
    "invalid_loyalty_numbers = invalid_dates['Loyalty#'].values\n",
    "invalid_flights = flightDB_copy[flightDB_copy['Loyalty#'].isin(invalid_loyalty_numbers)]\n",
    "invalid_flights_grouped = invalid_flights.groupby('Loyalty#').agg({\n",
    "    'NumFlights': 'sum',\n",
    "    'NumFlightsWithCompanions': 'sum',\n",
    "    'DistanceKM': 'sum',\n",
    "    'PointsAccumulated': 'sum'\n",
    "}).reset_index()\n",
    "invalid_flights_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac3fe35",
   "metadata": {},
   "source": [
    "Since we have relevant flight data for customers with invalid dates, we are going to assume that the customer had a previous membership, cancelled it, and then decided to become a member once again.\n",
    "\n",
    "For this reason, we're just going to replace these dates in `CancellationDate` with Na.\n",
    "\n",
    "*We're not adding this to the dropped rows count as we only did a replacement.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050a5783",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in invalid_dates:\n",
    "    customerDB_copy.loc[invalid_dates.index, 'CancellationDate'] = pd.NaT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b83676e",
   "metadata": {},
   "source": [
    "#### CustomerDB - FlightDB Relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9cfcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightDB_copy.shape[0]/customerDB_copy.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071f7efa",
   "metadata": {},
   "source": [
    "Every customer in the loyalty program should have a row for each one of the 36 months in the dataset. The number above shows that there are more months registered than customers. We'll take a look into this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07704371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each loyalty number, counting how many rows there are in the flights dataset\n",
    "flight_counts = flightDB_copy['Loyalty#'].value_counts()\n",
    "loyalty_over_36 = flight_counts[flight_counts > 36]\n",
    "print(f\"Number of Loyalty# with more than 36 rows: {len(loyalty_over_36)}\")\n",
    "\n",
    "# for each loyalty number, counting how many months are missing in the flights dataset\n",
    "months_per_loyalty = flightDB_copy.groupby('Loyalty#').size()\n",
    "missing_months = months_per_loyalty[months_per_loyalty < 36]\n",
    "print(f\"Number of Loyalty# with less than 36 months: {len(missing_months)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ca89a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing the rows for one Loyalty# with more than 36 rows\n",
    "flightDB_copy[flightDB_copy['Loyalty#'] == loyalty_over_36.index[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea5e2ba",
   "metadata": {},
   "source": [
    "For each repeated month, we'll keep the row with the less amount of null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6784e4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null_counts = flightDB_copy.notnull().sum(axis=1) # counting non-null values in each row\n",
    "row_to_keep = (flightDB_copy.assign(non_null_count=non_null_counts).groupby(['Loyalty#', 'Year', 'Month'])['non_null_count'].idxmax()) # getting the index of the row with the most non-null values for each Loyalty#, Year, Month\n",
    "flightDB_copy = flightDB_copy.loc[row_to_keep].copy() # keeping only the rows with the most non-null values\n",
    "\n",
    "# updating the dropped rows counter\n",
    "flight_dropped_rows = flight_original_rows - flightDB_copy.shape[0]\n",
    "flight_available_to_drop -= flight_dropped_rows\n",
    "\n",
    "print(f\"Total dropped rows: {flight_dropped_rows}, which is {(flight_dropped_rows/flight_original_rows)*100:.2f}% of the original dataset. We can still drop {flight_available_to_drop} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e129b8",
   "metadata": {},
   "source": [
    "*Side comment: the same percentage of rows was dropped when we were cleaning the duplicate customers... This could mean that the people dropped in this previous step had an already existing ID assigned to them when creating their account. This would have mixed their data with the other customer with the existing ID.*\n",
    "\n",
    "*There's no way of identifying which flight information is related to each one of these customers, so we'll need to work with what we have.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcbe9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each loyalty number, counting how many rows there are in the flights dataset\n",
    "flight_counts = flightDB_copy['Loyalty#'].value_counts()\n",
    "loyalty_over_36 = flight_counts[flight_counts > 36]\n",
    "print(f\"Number of Loyalty# with more than 36 rows: {len(loyalty_over_36)}\")\n",
    "\n",
    "# for each loyalty number, counting how many months are missing in the flights dataset\n",
    "months_per_loyalty = flightDB_copy.groupby('Loyalty#').size()\n",
    "missing_months = months_per_loyalty[months_per_loyalty < 36]\n",
    "print(f\"Number of Loyalty# with less than 36 months: {len(missing_months)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8d0957",
   "metadata": {},
   "source": [
    "### 2. <a id=\"4_2\">Variable Classification</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df3939f",
   "metadata": {},
   "source": [
    "*It makes no sense to make `Loyalty#` the index in the flightDB dataset as the same customer may have multiple flights.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37ad16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Loyalty# the index for the customerDB\n",
    "customerDB_copy.set_index('Loyalty#', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccfe509",
   "metadata": {},
   "source": [
    "We're going to categorise the different variables in both datasets.\n",
    "\n",
    "\n",
    "For `customerDB`, we'll have Nominal, Ordinal, Numerical, and DateTime variables. We're diving the categorical variables in Nominal and Ordinal because the latter will have its variables used for the Encoded category.\n",
    "\n",
    "For `flightDB`, we'll only have Numerical and DateTime variables since this dataset has no categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9198b557",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDB_copy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614b3e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_nominal_variables = ['Province or State', 'City', 'Gender', 'Location Code', 'Marital Status', 'EnrollmentType']\n",
    "customer_ordinal_variables = ['Education', 'LoyaltyStatus']\n",
    "customer_numerical_variables = customerDB_copy.select_dtypes(include=[np.number]).columns.tolist()\n",
    "customer_datetime_variables = customerDB_copy.select_dtypes(include=['datetime']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af08cdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding ordinal variables\n",
    "education_map = {'Bachelor':3, 'College':2, 'High School or Below':1, 'Doctor':5, 'Master':4}\n",
    "loyaltystatus_map = {'Star':1, 'Nova':2, 'Aurora':3}\n",
    "for variable in customer_ordinal_variables:\n",
    "    if variable == 'Education':\n",
    "        customerDB_copy['Education Encoded'] = customerDB_copy[variable].map(education_map)\n",
    "    elif variable == 'LoyaltyStatus':\n",
    "        customerDB_copy['LoyaltyStatus Encoded'] = customerDB_copy[variable].map(loyaltystatus_map)\n",
    "customer_ordinal_variables_encoded = ['Education Encoded', 'LoyaltyStatus Encoded'] # New list of encoded ordinal variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad953de",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightDB_copy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c819b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_numerical_variables = flightDB_copy.select_dtypes(include=['float']).columns.tolist()\n",
    "flight_date_variables = ['Year', 'Month']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd890af9",
   "metadata": {},
   "source": [
    "*We could change `Year` and `Month` into a datetime column, but we figured they would be more useful for analysis as separate variables.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daafe462",
   "metadata": {},
   "source": [
    "### 3. <a id=\"4_3\">Summary Statistics</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a29321",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDB_copy[customer_ordinal_variables].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b7775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDB_copy[customer_numerical_variables].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cef308b",
   "metadata": {},
   "source": [
    "`Customer Lifetime Value` seems to have outliers on the right, as the mean is higher than the median. This means that *some* customers have a high Lifetime Value, but the majority doesn't. It makes sense from a logical point of view as we don't see most people having frequent flights. However, we'll try to understand how this variable relates to the rest in the Data Visualisation section.\n",
    "\n",
    "As said previously, in clustering, we will try to understand if there's a pattern in the missing values in `Income`. *(i.e. If they're missing completely at random, at random or not at random)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a09b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightDB_copy[flight_numerical_variables].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83e8966",
   "metadata": {},
   "source": [
    "In the Flights dataset, we can see that most variables have many rows with null values.\n",
    "\n",
    "We can conclude that 25% of the registered *months by customer* have no flights. This could mean that some months have few or no flights or that some customers have no flights in most or all months.\n",
    "\n",
    "The `NumFlightsWithCompanions` variable is clearly related to the `NumFlights`, as one indicates the number of flights *with companions* and the other the number of flights the customer had in the month in total, respectively, so it makes sense that it has even more null values (at least 50%).\n",
    "\n",
    "The remaining values follow the same logic. They're all related to the total number of flights, so it makes sense that at least 25% of their values are null since 25% of `NumFlights`'s values is also null.\n",
    "\n",
    "`PointsRedeemed` and `DollarCostPointsRedeemed` also show that most customers (75% or more) don't redeem their available points.\n",
    "\n",
    "Overall, we can conclude that every numerical variable of this dataset is left-skewed as the mean is much higher than the median in every case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c90bb5",
   "metadata": {},
   "source": [
    "### 4. <a id=\"4_4\">Data Visualisation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bd1ea5",
   "metadata": {},
   "source": [
    "In this section, we’ll visually explore the data to understand the distributions of each variable and the relationships between them. We’ll also use this step to confirm and inspect any outliers identified during the summary statistics stage.\n",
    "\n",
    "Once we’ve gained a solid understanding of the existing variables, we’ll move on to the feature engineering phase, where we’ll create new features and aggregations based on these insights. Afterwards, we’ll do a brief round of statistical and visual checks to ensure the new variables behave as expected and add meaningful value to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4426495",
   "metadata": {},
   "source": [
    "#### Numerical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75029a30",
   "metadata": {},
   "source": [
    "#### Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a944cdba",
   "metadata": {},
   "source": [
    "#### Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8135b4d",
   "metadata": {},
   "source": [
    "##### Numerical & Numerical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adde3d09",
   "metadata": {},
   "source": [
    "##### Categorical & Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b917fd24",
   "metadata": {},
   "source": [
    "##### Numerical & Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1a72e1",
   "metadata": {},
   "source": [
    "### 5. <a id=\"4_5\">Feature Engineering</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a702b6b",
   "metadata": {},
   "source": [
    "To use data on the customer level, we'll have to calculate the total and average values on the flight dataset since we currently have 36 rows (12 months for 3 years) for each customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f30bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate all flight metrics at once and merge in a single operation\n",
    "agg = (flightDB_copy.groupby('Loyalty#')[flight_numerical_variables].agg(['sum', 'mean']))\n",
    "\n",
    "new_cols = []\n",
    "for col, stat in agg.columns:\n",
    "    if stat == 'sum':\n",
    "        prefix = 'Total'\n",
    "    else:\n",
    "        prefix = 'Average'\n",
    "    new_cols.append(f\"{prefix} {col}\")\n",
    "agg.columns = new_cols\n",
    "\n",
    "customerDB_copy = customerDB_copy.merge(agg, left_index=True, right_index=True, how='left')\n",
    "\n",
    "customerDB_copy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71062827",
   "metadata": {},
   "source": [
    "#\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a216c3e4",
   "metadata": {},
   "source": [
    "## 5. <a id=\"5\">Clustering</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7b1040",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766b1856",
   "metadata": {},
   "source": [
    "## 6. <a id=\"6\">Evaluation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb88a501",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283a772c",
   "metadata": {},
   "source": [
    "## 7. <a id=\"7\">Suggestions</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adf9c75",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fall2526",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
